{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb82f5c7",
   "metadata": {},
   "source": [
    "# **I. Model Selection & Baseline Leaderboard**\n",
    "\n",
    "## **I.1. Objective & Experiment Design**\n",
    "\n",
    "The objective of this phase is to establish a **performance baseline** using default algorithms. This \"Leaderboard\" serves two critical functions:\n",
    "\n",
    "1.  **Complexity Justification:** It determines whether complex non-linear models (like XGBoost) provide a statistically significant advantage over simpler linear models (Logistic Regression).\n",
    "2.  **Overfitting Detection:** By simultaneously measuring performance on the **Training Set** and the **Validation Set**, we identify models that \"memorize\" the data rather than generalizing the underlying cluster logic.\n",
    "\n",
    "**The Candidate Algorithms:**\n",
    "\n",
    "*   **Logistic Regression:** A linear baseline. If this performs well, the cluster boundaries are simple and linear.\n",
    "*   **Random Forest:** A bagging ensemble. It captures non-linear interactions and serves as the direct comparator to Group 1's approach.\n",
    "*   **XGBoost / LightGBM:** Gradient boosting machines. These are the current industry standard for tabular data, expected to handle the \"Augmented Features\" (like delivery delays) most effectively.\n",
    "\n",
    "## **I.2. Environment Setup & Metric Definition**\n",
    "To ensure rigorous evaluation, we define a standardized **Evaluation Function** that will be applied identically to all models.\n",
    "\n",
    "**Key Metrics Selection:**\n",
    "*   **F1-Score (Weighted):** The primary success metric. It accounts for the **Class Imbalance** (Cluster 2 is only 3%) by weighting the score of each class by its support (number of true instances).\n",
    "*   **Log Loss (Cross-Entropy):** Measures the **confidence** of predictions. A model that predicts the correct class with 51% probability is \"worse\" than one predicting with 90% probability, even if accuracy is identical.\n",
    "*   **ROC-AUC (One-vs-Rest):** Measures the model's ability to distinguish between classes across different probability thresholds.\n",
    "\n",
    "\n",
    "**Code Implementation: Setup & Metrics**\n",
    "\n",
    "**Purpose:** Import necessary libraries, load the stratified datasets created in ***Data Engineering*** section, and define a reusable function to calculate and display the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d192689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn Models & Metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, log_loss, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622b7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Loading Stratified Data ---\n",
      "    Train Shape: (65350, 10)\n",
      "    Val Shape:   (14004, 10)\n",
      "    Test Shape:  (14004, 10)\n",
      "--- [END] Data Loaded Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "def load_stratified_data():\n",
    "    \"\"\"\n",
    "    Loads the 6 split files generated in Task 3.\n",
    "    Returns X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \"\"\"\n",
    "    print(\"--- [START] Loading Stratified Data ---\")\n",
    "    try:\n",
    "        X_train = pd.read_csv(f\"{DATA_DIR}X_train.csv\")\n",
    "        y_train = pd.read_csv(f\"{DATA_DIR}y_train.csv\").squeeze() # squeeze to convert DF to Series\n",
    "        X_val = pd.read_csv(f\"{DATA_DIR}X_val.csv\")\n",
    "        y_val = pd.read_csv(f\"{DATA_DIR}y_val.csv\").squeeze()\n",
    "        X_test = pd.read_csv(f\"{DATA_DIR}X_test.csv\")\n",
    "        y_test = pd.read_csv(f\"{DATA_DIR}y_test.csv\").squeeze()\n",
    "        \n",
    "        print(f\"    Train Shape: {X_train.shape}\")\n",
    "        print(f\"    Val Shape:   {X_val.shape}\")\n",
    "        print(f\"    Test Shape:  {X_test.shape}\")\n",
    "        print(\"--- [END] Data Loaded Successfully ---\\n\")\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    except FileNotFoundError:\n",
    "        print(\"    [ERROR] Split files not found. Please run Task 3 first.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Calculates F1-Weighted, Log Loss, and ROC-AUC for a given model and dataset.\n",
    "    Returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    # 1. Generate Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)\n",
    "    \n",
    "    # 2. Calculate Metrics\n",
    "    # F1-Weighted handles class imbalance\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    \n",
    "    # Log Loss requires probability estimates\n",
    "    try:\n",
    "        ll = log_loss(y, y_prob)\n",
    "    except ValueError:\n",
    "        # Fallback if classes are missing in a small split (rare with stratification)\n",
    "        ll = np.nan\n",
    "        \n",
    "    # ROC-AUC (One-vs-Rest) for Multiclass\n",
    "    try:\n",
    "        auc = roc_auc_score(y, y_prob, multi_class='ovr', average='weighted')\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    # 3. Print Results\n",
    "    print(f\"[{dataset_name} Performance]\")\n",
    "    print(f\"    F1-Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"    ROC-AUC (OvR):       {auc:.4f}\")\n",
    "    print(f\"    Log Loss:            {ll:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"F1_Weighted\": f1,\n",
    "        \"ROC_AUC\": auc,\n",
    "        \"Log_Loss\": ll\n",
    "    }\n",
    "\n",
    "# Execute Loading\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_stratified_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde282a",
   "metadata": {},
   "source": [
    "## **I.3. Baseline Models: Logistic Regression & Random Forest**\n",
    "\n",
    "*   **Logistic Regression:**\n",
    "    *   **Role:** The \"Simplicity Test.\"\n",
    "    *   **Hypothesis:** If the clusters created by K-Means are geometrically distinct (e.g., separated by clear hyperplanes), this simple linear model should perform surprisingly well. If it fails (low F1 score), it confirms the relationships between inputs (like `credit_card_usage`) and clusters are **non-linear**.\n",
    "    *   **Configuration:** We use `max_iter=1000` to ensure the solver converges, as our dataset has 93k rows.\n",
    "\n",
    "*   **Random Forest Classifier:**\n",
    "    *   **Role:** The \"Direct Comparator.\"\n",
    "    *   **Hypothesis:** This mimics Group 1's approach but applied to our **stratified** data. Random Forest handles non-linearity well but is prone to **overfitting** (high Train score, low Val score) if trees are allowed to grow too deep.\n",
    "    *   **Configuration:** We use a `random_state` seed for reproducibility.\n",
    "\n",
    "**Code Implementation: Training LR and RF**\n",
    "\n",
    "**Purpose:** Train both models on `X_train`, then evaluate them on **both** `X_train` (to check for memorization) and `X_val` (to check for generalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb407e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 1: Logistic Regression ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_Python\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "d:\\_Python\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 3000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=3000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Weighted): 0.9505\n",
      "    ROC-AUC (OvR):       0.9942\n",
      "    Log Loss:            0.1144\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Weighted): 0.9506\n",
      "    ROC-AUC (OvR):       0.9939\n",
      "    Log Loss:            0.1184\n",
      "\n",
      "=== Model 2: Random Forest (Default) ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Weighted): 1.0000\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0075\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Weighted): 0.9951\n",
      "    ROC-AUC (OvR):       0.9999\n",
      "    Log Loss:            0.0229\n"
     ]
    }
   ],
   "source": [
    "# Store results for the Leaderboard comparison later\n",
    "leaderboard = []\n",
    "\n",
    "# --- 1. LOGISTIC REGRESSION (Linear Baseline) ---\n",
    "print(\"\\n=== Model 1: Logistic Regression ===\")\n",
    "# Initialize\n",
    "lr_model = LogisticRegression(\n",
    "    multi_class='multinomial', \n",
    "    solver='lbfgs', \n",
    "    max_iter=3000, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on Train (Check for Underfitting)\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_lr = evaluate_model(lr_model, X_train, y_train, dataset_name=\"Train\")\n",
    "\n",
    "# Evaluate on Validation (Check for Generalization)\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_lr = evaluate_model(lr_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# Record Results\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Train_F1\": train_metrics_lr[\"F1_Weighted\"],\n",
    "    \"Val_F1\": val_metrics_lr[\"F1_Weighted\"],\n",
    "    \"Val_LogLoss\": val_metrics_lr[\"Log_Loss\"]\n",
    "})\n",
    "\n",
    "\n",
    "# --- 2. RANDOM FOREST (Bagging Ensemble) ---\n",
    "print(\"\\n=== Model 2: Random Forest (Default) ===\")\n",
    "# Initialize\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on Train\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_rf = evaluate_model(rf_model, X_train, y_train, dataset_name=\"Train\")\n",
    "\n",
    "# Evaluate on Validation\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_rf = evaluate_model(rf_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# Record Results\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"Train_F1\": train_metrics_rf[\"F1_Weighted\"],\n",
    "    \"Val_F1\": val_metrics_rf[\"F1_Weighted\"],\n",
    "    \"Val_LogLoss\": val_metrics_rf[\"Log_Loss\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4147b",
   "metadata": {},
   "source": [
    "**Analysis of Output**\n",
    "\n",
    "1.  **The Logistic Regression Warning:**\n",
    "    *   `lbfgs failed to converge`: This is expected. Even with 3000 iterations, the complex high-dimensional surface (10 features, 4 classes) makes it hard for a linear solver to find the absolute global minimum.\n",
    "    *   **Performance:** Despite non-convergence, it hit **95% F1-Score**. This proves that the clusters have **very strong linear separation**. This is a key finding: *K-Means created mathematically distinct groups, so even a simple line can separate them well.*\n",
    "\n",
    "2.  **The Random Forest \"Perfect Score\":**\n",
    "    *   **Train F1 = 1.0000**: The model has perfectly memorized the training data. This is textbook **overfitting**.\n",
    "    *   **Validation F1 = 0.9951**: However, the validation score is also near perfect.\n",
    "    *   **Implication:** This confirms the \"Surrogate Model\" nature of the task. Because $Y$ (Cluster) was created from $X$ (Features), the mapping is deterministic. The Random Forest has successfully reverse-engineered the K-Means logic.\n",
    "\n",
    "**Modification:** We do not need to \"fix\" the Logistic Regression convergence because it is just a baseline comparison. The fact that it struggles while RF succeeds validates moving to tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a33481",
   "metadata": {},
   "source": [
    "## **I.4. Gradient Boosting Challengers: XGBoost & LightGBM**\n",
    "\n",
    "To complete the baseline assessment, we deploy two industry-standard Gradient Boosting machines. Unlike Random Forest (which builds trees in parallel), these models build trees **sequentially**, with each new tree correcting the errors of the previous one.\n",
    "\n",
    "*   **XGBoost (eXtreme Gradient Boosting):**\n",
    "    *   **Strength:** Known for precision and regularization (preventing overfitting).\n",
    "    *   **Hypothesis:** It should provide the best **Log Loss** (probability calibration) of all models. XGBoost should match Random Forest's accuracy (99%+) but potentially with lower **Log Loss** (better probability confidence).\n",
    "*   **LightGBM (Light Gradient Boosting Machine):**\n",
    "    *   **Strength:** Optimized for speed and efficiency using histogram-based learning.\n",
    "    *   **Hypothesis:** It serves as a \"Speed Test.\" If it matches XGBoost's accuracy but trains 5x faster, it becomes the preferred candidate for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 3: XGBoost Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Weighted): 1.0000\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0021\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Weighted): 0.9964\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0092\n",
      "\n",
      "=== Model 4: LightGBM Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Weighted): 0.9999\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0044\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Weighted): 0.9967\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0097\n",
      "\n",
      "=== ðŸ† Phase A Leaderboard: Default Models ===\n",
      "| Model               |   Train_F1 |   Val_F1 |   Val_LogLoss |\n",
      "|:--------------------|-----------:|---------:|--------------:|\n",
      "| LightGBM            |   0.999908 | 0.996715 |    0.00968221 |\n",
      "| XGBoost             |   1        | 0.996358 |    0.00920968 |\n",
      "| Random Forest       |   1        | 0.995073 |    0.0229231  |\n",
      "| Logistic Regression |   0.950453 | 0.950567 |    0.118396   |\n"
     ]
    }
   ],
   "source": [
    "# --- 3. XGBOOST (Gradient Boosting) ---\n",
    "print(\"\\n=== Model 3: XGBoost Classifier ===\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_xgb = evaluate_model(xgb_model, X_train, y_train, dataset_name=\"Train\")\n",
    "\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_xgb = evaluate_model(xgb_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "leaderboard.append({\n",
    "    \"Model\": \"XGBoost\",\n",
    "    \"Train_F1\": train_metrics_xgb[\"F1_Weighted\"],\n",
    "    \"Val_F1\": val_metrics_xgb[\"F1_Weighted\"],\n",
    "    \"Val_LogLoss\": val_metrics_xgb[\"Log_Loss\"]\n",
    "})\n",
    "\n",
    "\n",
    "# --- 4. LIGHTGBM (High-Efficiency Boosting) ---\n",
    "print(\"\\n=== Model 4: LightGBM Classifier ===\")\n",
    "# Force verbosity=-1 to suppress internal warnings\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_lgb = evaluate_model(lgb_model, X_train, y_train, dataset_name=\"Train\")\n",
    "\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_lgb = evaluate_model(lgb_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "leaderboard.append({\n",
    "    \"Model\": \"LightGBM\",\n",
    "    \"Train_F1\": train_metrics_lgb[\"F1_Weighted\"],\n",
    "    \"Val_F1\": val_metrics_lgb[\"F1_Weighted\"],\n",
    "    \"Val_LogLoss\": val_metrics_lgb[\"Log_Loss\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ad5ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVED] Phase A Leaderboard saved to: data/phase_a_leaderboard.csv\n",
      "| Model               |   Train_F1 |   Val_F1 |   Val_LogLoss |\n",
      "|:--------------------|-----------:|---------:|--------------:|\n",
      "| LightGBM            |   0.999908 | 0.996715 |    0.00968221 |\n",
      "| XGBoost             |   1        | 0.996358 |    0.00920968 |\n",
      "| Random Forest       |   1        | 0.995073 |    0.0229231  |\n",
      "| Logistic Regression |   0.950453 | 0.950567 |    0.118396   |\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE PHASE A RESULTS ---\n",
    "leaderboard_path = f\"{DATA_DIR}phase_a_leaderboard.csv\"\n",
    "df_leaderboard = pd.DataFrame(leaderboard)\n",
    "\n",
    "# Save to CSV\n",
    "df_leaderboard.to_csv(leaderboard_path, index=False)\n",
    "print(f\"\\n[SAVED] Phase A Leaderboard saved to: {leaderboard_path}\")\n",
    "\n",
    "# Display the table (Sorted by Val_F1)\n",
    "print(df_leaderboard.sort_values(by=\"Val_F1\", ascending=False).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222384a0",
   "metadata": {},
   "source": [
    "## **I.5. Conclusion: Baseline Model Selection**\n",
    "\n",
    "We successfully evaluated four distinct algorithms on the stratified datasets. The goal was to select a single \"Champion\" model for hyperparameter tuning based on **Generalization** (Validation F1) and **Confidence** (Log Loss).\n",
    "\n",
    "**The Leaderboard:**\n",
    "\n",
    "| Rank | Model | Val F1-Score | Val Log-Loss | Interpretation |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **1** | **LightGBM** | **0.9967** | 0.0097 | **The Winner.** Highest accuracy and extremely fast convergence. |\n",
    "| **2** | **XGBoost** | 0.9964 | **0.0092** | Very close second. Slightly better calibrated probabilities (lower Log Loss) but marginally lower F1. |\n",
    "| **3** | **Random Forest** | 0.9951 | 0.0229 | Excellent accuracy, but significantly worse Log Loss (0.02 vs 0.009), indicating it is \"less sure\" of its predictions than boosting models. |\n",
    "| **4** | **Logistic Regression** | 0.9506 | 0.1184 | The linear baseline performed surprisingly well (95%), proving the clusters have strong linear separability, but it cannot capture the edge cases like the tree models. |\n",
    "\n",
    "**Key Findings:**\n",
    "1.  **The \"Surrogate\" Validation:** All tree-based models achieved >99.5% accuracy. This effectively proves that **Group 1's clusters are deterministic.** The clusters are not random noise; they follow a strict logic based on the input features, which the models successfully reverse-engineered.\n",
    "2.  **Boosting > Bagging:** Both Gradient Boosting methods (LightGBM, XGBoost) outperformed Random Forest in **Log Loss** by a factor of 2. They are not just guessing the right class; they are assigning it with near-100% probability.\n",
    "3.  **Selection Decision:** We select **XGBoost** for the Tuning Phase.\n",
    "    *   *Why not LightGBM?* Although LightGBM won on F1 by a tiny margin (0.0003), XGBoost has the **lowest Log Loss (0.0092)** and creates the most robust **SHAP plots**, which is our ultimate goal. (XGBoost's `TreeExplainer` is the gold standard for interpretability).\n",
    "\n",
    "$\\Rightarrow$ **Next Step:** We proceed to **Phase B: Hyperparameter Tuning of XGBoost** to prevent overfitting and ensure the model remains stable when we strip away features later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a533f2",
   "metadata": {},
   "source": [
    "# **II. Hyperparameter Tuning (XGBoost)**\n",
    "\n",
    "## **II.1. Tuning Strategy: Bayesian Optimization**\n",
    "\n",
    "Instead of a brute-force Grid Search (which is slow and inefficient), we employ **Tree-structured Parzen Estimator (TPE)** via the **Optuna** framework.\n",
    "\n",
    "*   **The Objective:** Minimize **Log Loss** on the Validation Set.\n",
    "    *   *Why Log Loss?* F1-Score is a \"step function\" (hard classes), making it difficult for an optimizer to find gradients. Log Loss is continuous and penalizes the model for being \"uncertain,\" providing a smoother path to the global optimum.\n",
    "*   **The Hyperparameter Search Space:**\n",
    "    *   **Structure:** `max_depth` (Tree complexity), `min_child_weight` (Leaf node minimum mass).\n",
    "    *   **Regularization:** `gamma` (Split threshold), `reg_alpha` (L1), `reg_lambda` (L2).\n",
    "    *   **Sampling:** `subsample` (Rows), `colsample_bytree` (Features).\n",
    "    *   **Learning:** `learning_rate` (Step size), `n_estimators` (Number of trees).\n",
    "\n",
    "## **II.2. Code Implementation: Optuna Study & Final Model Training**\n",
    "\n",
    "**Purpose:** Run 20 trials to find the optimal configuration that minimizes Log Loss without overfitting. Train the definitive `best_xgb_model` using the parameters found above and perform a final check on the Validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd5bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Hyperparameter Tuning with Optuna ---\n",
      "--- [SAVED] Best parameters saved to data/best_xgb_params.json ---\n",
      "\n",
      "--- [RESULT] Best Parameters Loaded ---\n",
      "    max_depth: 9\n",
      "    min_child_weight: 7\n",
      "    learning_rate: 0.12688996739620345\n",
      "    n_estimators: 846\n",
      "    gamma: 0.032537918839191105\n",
      "    reg_alpha: 4.714958436497499\n",
      "    reg_lambda: 3.237417655214124\n",
      "    subsample: 0.8148533995968621\n",
      "    colsample_bytree: 0.946760070343784\n",
      "\n",
      "=== Training Final Champion Model ===\n",
      "  > Evaluating Best Model on VALIDATION set...\n",
      "[Final Validation Performance]\n",
      "    F1-Score (Weighted): 0.9964\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0107\n",
      "\n",
      "[Conclusion] Tuning Improvement:\n",
      "    Default Log Loss: 0.0092\n",
      "    Tuned Log Loss:   0.01073\n",
      "    Performance Lift: -16.64%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import os\n",
    "# from sklearn.metrics import log_loss\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PARAMS_FILE = \"data/best_xgb_params.json\"\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_prob)\n",
    "\n",
    "# --- EXECUTION LOGIC ---\n",
    "if os.path.exists(PARAMS_FILE):\n",
    "    print(f\"--- [INFO] Found saved parameters in {PARAMS_FILE}. Skipping optimization. ---\")\n",
    "    with open(PARAMS_FILE, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "else:\n",
    "    print(\"--- [START] Hyperparameter Tuning with Optuna ---\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(PARAMS_FILE, 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(f\"--- [SAVED] Best parameters saved to {PARAMS_FILE} ---\")\n",
    "\n",
    "print(\"\\n--- [RESULT] Best Parameters Loaded ---\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# --- TRAIN FINAL MODEL ---\n",
    "print(\"\\n=== Training Final Champion Model ===\")\n",
    "# Ensure static params are added back (as they aren't optimized)\n",
    "best_params.update({\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 4,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# --- FINAL VALIDATION CHECK ---\n",
    "print(\"  > Evaluating Best Model on VALIDATION set...\")\n",
    "final_metrics = evaluate_model(final_model, X_val, y_val, dataset_name=\"Final Validation\")\n",
    "\n",
    "# Calculate Lift\n",
    "default_logloss = 0.0092 # From Phase A\n",
    "lift = ((default_logloss - final_metrics['Log_Loss']) / default_logloss) * 100\n",
    "\n",
    "print(f\"\\n[Conclusion] Tuning Improvement:\")\n",
    "print(f\"    Default Log Loss: {default_logloss}\")\n",
    "print(f\"    Tuned Log Loss:   {final_metrics['Log_Loss']:.5f}\")\n",
    "print(f\"    Performance Lift: {lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3533c",
   "metadata": {},
   "source": [
    "The output is **correct**, but it reveals a critical nuance: **The Tuning actually made the model slightly \"worse\"** (Negative Lift: -16.64%).\n",
    "\n",
    "**Why did this happen?**\n",
    "*   **Default Log Loss (0.0092):** The default XGBoost is extremely aggressive (unconstrained depth, no regularization). It fits the data perfectly.\n",
    "*   **Tuned Log Loss (0.0107):** Our Optuna search added **Regularization** (`reg_alpha`, `reg_lambda`, `gamma`).\n",
    "*   **Interpretation:** The tuned model is \"less confident\" (higher log loss) but **more robust**. The default model likely \"overfit\" the probabilities (e.g., predicting 99.99% certainty), while the tuned model is more conservative (e.g., predicting 98% certainty).\n",
    "*   **Is this bad?** No. In a production environment, we prefer the regularized model because it is less likely to break on unseen data. However, for a *Surrogate Model* whose only job is to mimic the clustering logic, the \"Default\" model might actually be superior because the clustering logic is static.\n",
    "\n",
    "\n",
    "The previous tuning result failed to beat the default because the **Search Space was too conservative**. It forced the model to simplify itself, whereas the \"Default\" model was allowed to be complex.\n",
    "\n",
    "**The Optimization Strategy: \"Beat the Default\"**\n",
    "\n",
    "1.  **Enqueue the Default:** We will explicitly tell Optuna to try the Default Parameters *first*. This guarantees the result will be **at least as good** as the default. It mathematically cannot be worse.\n",
    "2.  **Relax Constraints:** We will allow `max_depth` to go deeper (up to 15) and allow regularization to drop to effectively zero.\n",
    "3.  **Increase Trials:** Bump to **50 trials** to explore the space more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382278bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Hyperparameter Tuning (Aggressive) ---\n",
      "--- [SAVED] Best parameters saved to data/best_xgb_params_v2.json ---\n",
      "\n",
      "--- [RESULT] Best Parameters Loaded ---\n",
      "    max_depth: 12\n",
      "    min_child_weight: 4\n",
      "    learning_rate: 0.033034837496608356\n",
      "    n_estimators: 1321\n",
      "    gamma: 1.47721632417482e-07\n",
      "    reg_alpha: 0.19108536698828876\n",
      "    reg_lambda: 8.427536862316056e-05\n",
      "    subsample: 0.8655603970394049\n",
      "    colsample_bytree: 0.9675651912854055\n",
      "\n",
      "=== Training Final Champion Model ===\n",
      "  > Evaluating Best Model on VALIDATION set...\n",
      "[Final Validation Performance]\n",
      "    F1-Score (Weighted): 0.9971\n",
      "    ROC-AUC (OvR):       1.0000\n",
      "    Log Loss:            0.0086\n",
      "\n",
      "[Conclusion] Tuning Improvement:\n",
      "    Default Log Loss: 0.0092\n",
      "    Tuned Log Loss:   0.00863\n",
      "    Performance Lift: 6.17%\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "PARAMS_FILE = \"data/best_xgb_params_v2.json\" # New file name to avoid conflict\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        \n",
    "        # BROADER Search Space\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 15), # Allow deeper trees\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n",
    "        \n",
    "        # LOWER Regularization Floor (Allowing it to be 0 like default)\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 2.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 2.0, log=True),\n",
    "        \n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_prob)\n",
    "\n",
    "# --- EXECUTION LOGIC ---\n",
    "if os.path.exists(PARAMS_FILE):\n",
    "    print(f\"--- [INFO] Found saved parameters in {PARAMS_FILE}. Skipping optimization. ---\")\n",
    "    with open(PARAMS_FILE, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "else:\n",
    "    print(\"--- [START] Hyperparameter Tuning (Aggressive) ---\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # 1. TRICK: Tell Optuna to try the \"Default-ish\" params first\n",
    "    # This ensures we start with a strong baseline\n",
    "    study.enqueue_trial({\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'learning_rate': 0.3,\n",
    "        'n_estimators': 100,\n",
    "        'gamma': 1e-8,\n",
    "        'reg_alpha': 1e-8,\n",
    "        'reg_lambda': 1.0,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0\n",
    "    })\n",
    "    \n",
    "    # 2. Run Optimization\n",
    "    study.optimize(objective, n_trials=30) # Increased to 30 for better coverage\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(PARAMS_FILE, 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(f\"--- [SAVED] Best parameters saved to {PARAMS_FILE} ---\")\n",
    "\n",
    "print(\"\\n--- [RESULT] Best Parameters Loaded ---\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# --- TRAIN FINAL MODEL ---\n",
    "print(\"\\n=== Training Final Champion Model ===\")\n",
    "best_params.update({\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 4,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# --- FINAL VALIDATION CHECK ---\n",
    "print(\"  > Evaluating Best Model on VALIDATION set...\")\n",
    "final_metrics = evaluate_model(final_model, X_val, y_val, dataset_name=\"Final Validation\")\n",
    "\n",
    "# Calculate Lift\n",
    "default_logloss = 0.0092 # From Phase A\n",
    "lift = ((default_logloss - final_metrics['Log_Loss']) / default_logloss) * 100\n",
    "\n",
    "print(f\"\\n[Conclusion] Tuning Improvement:\")\n",
    "print(f\"    Default Log Loss: {default_logloss}\")\n",
    "print(f\"    Tuned Log Loss:   {final_metrics['Log_Loss']:.5f}\")\n",
    "print(f\"    Performance Lift: {lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdcf94",
   "metadata": {},
   "source": [
    "### **II.3. Conclusion: Hyperparameter Optimization**\n",
    "\n",
    "We moved beyond the default settings to rigorously optimize the XGBoost architecture using **Bayesian Optimization (Optuna)**. By expanding the search space and reducing constraints, we allowed the model to find a more complex and precise configuration.\n",
    "\n",
    "**Optimization Results:**\n",
    "*   **The Winning Configuration:** The algorithm converged on a \"Deep & Slow\" learning strategy:\n",
    "    *   **`max_depth: 12`**: Significantly deeper than the default (6), allowing the model to capture highly complex, non-linear interaction effects between features.\n",
    "    *   **`learning_rate: 0.033`**: A low learning rate (vs default 0.3) combined with high **`n_estimators: 1321`** ensures precise convergence without overshooting the minima.\n",
    "    *   **`gamma` & `reg_alpha` $\\approx$ 0**: The optimizer determined that strict regularization was unnecessary; the signal in the data is strong and clean.\n",
    "\n",
    "**Performance Impact:**\n",
    "\n",
    "| Metric | Default Model | Tuned Model | Improvement |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Log Loss (Confidence)** | 0.00920 | **0.00863** | **+6.17% Lift** |\n",
    "| **F1-Score (Accuracy)** | 0.9964 | **0.9971** | **+0.07% Lift** |\n",
    "\n",
    "**Key Findings:**\n",
    "*   **Precision Engineering:** The positive lift of **6.17%** in Log Loss proves that the default model was slightly \"under-fitted\" to the nuances of the clusters. The tuned model is statistically more confident in its predictions.\n",
    "*   **Surrogate Validity:** With an F1-Score of **99.71%**, the model effectively becomes a \"Digital Twin\" of the clustering logic. We can now be certain that explaining this model is equivalent to explaining the clusters themselves.\n",
    "\n",
    "$\\Rightarrow$ **Verdict:** We deploy the **Tuned XGBoost (v2)** as the finalized engine for the Feature Importance & SHAP analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac4f2d",
   "metadata": {},
   "source": [
    "# **III. Visualization (Presentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7112cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Figure 4 saved to figures/figure4_model_leaderboard.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Val_LogLoss=%{marker.color}<br>Model=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "Iv32deCcgT9IUPwYc9eCP+hqK/aX3YM/F0hQ/Bhzlz+srdhfdk++Pw==",
           "dtype": "f8"
          },
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "h",
         "showlegend": false,
         "textposition": "auto",
         "texttemplate": "%{x:.4f}",
         "type": "bar",
         "x": {
          "bdata": "Iv32deCcgT9IUPwYc9eCP+hqK/aX3YM/F0hQ/Bhzlz+srdhfdk++Pw==",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": [
          "XGBoost (Tuned v2)",
          "XGBoost (Default)",
          "LightGBM",
          "Random Forest",
          "Logistic Regression"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "coloraxis": {
         "colorbar": {
          "tickfont": {
           "size": 14
          },
          "title": {
           "font": {
            "size": 16
           },
           "side": "right",
           "text": "Val LogLoss"
          }
         },
         "colorscale": [
          [
           0,
           "#08306b"
          ],
          [
           1,
           "#bdd7e7"
          ]
         ],
         "showscale": true
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "l": 230,
         "r": 100
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 22
         },
         "text": "<b>Model Confidence (Log Loss): Lower is Better</b><br><sup>The Tuned XGBoost achieves the highest certainty (Lowest Error).</sup>"
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 16
          },
          "text": "<b>Validation Log Loss (Lower = Better)</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 16
          },
          "text": "<b>Model</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Figure 5 saved to figures/figure5_tuning_lift.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "XGBoost (Tuned v2)"
          ]
         ],
         "hovertemplate": "Model=%{customdata[0]}<br>Val_LogLoss=%{x}<br>Val_F1=%{y}<br>size=%{marker.size}<extra></extra>",
         "legendgroup": "XGBoost (Tuned v2)",
         "marker": {
          "color": "#d62728",
          "opacity": 0.75,
          "size": {
           "bdata": "Rg==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.175,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "XGBoost (Tuned v2)",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "Iv32deCcgT8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "Y+5aQj7o7z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "XGBoost (Default)"
          ]
         ],
         "hovertemplate": "Model=%{customdata[0]}<br>Val_LogLoss=%{x}<br>Val_F1=%{y}<br>size=%{marker.size}<extra></extra>",
         "legendgroup": "XGBoost (Default)",
         "marker": {
          "color": "#1f77b4",
          "opacity": 0.75,
          "size": {
           "bdata": "Mg==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.175,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "XGBoost (Default)",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "SFD8GHPXgj8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "NIC3QILi7z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "LightGBM"
          ]
         ],
         "hovertemplate": "Model=%{customdata[0]}<br>Val_LogLoss=%{x}<br>Val_F1=%{y}<br>size=%{marker.size}<extra></extra>",
         "legendgroup": "LightGBM",
         "marker": {
          "color": "#2ca02c",
          "opacity": 0.75,
          "size": {
           "bdata": "Mg==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.175,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "LightGBM",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "6Gor9pfdgz8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "24r9Zffk7z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Random Forest"
          ]
         ],
         "hovertemplate": "Model=%{customdata[0]}<br>Val_LogLoss=%{x}<br>Val_F1=%{y}<br>size=%{marker.size}<extra></extra>",
         "legendgroup": "Random Forest",
         "marker": {
          "color": "#9467bd",
          "opacity": 0.75,
          "size": {
           "bdata": "LQ==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.175,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Random Forest",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "F0hQ/Bhzlz8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "ufyH9NvX7z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Logistic Regression"
          ]
         ],
         "hovertemplate": "Model=%{customdata[0]}<br>Val_LogLoss=%{x}<br>Val_F1=%{y}<br>size=%{marker.size}<extra></extra>",
         "legendgroup": "Logistic Regression",
         "marker": {
          "color": "#E7B142",
          "opacity": 0.75,
          "size": {
           "bdata": "LQ==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.175,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Logistic Regression",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "rK3YX3ZPvj8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "s3vysFBr7j8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "left",
          "font": {
           "color": "#2c3e50",
           "size": 16
          },
          "showarrow": false,
          "text": "<span style='color:#d62728; font-size:20px;'>â—</span> <b>ðŸ† XGBoost (Tuned v2)</b>",
          "x": 1.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 1,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "left",
          "font": {
           "color": "#2c3e50",
           "size": 16
          },
          "showarrow": false,
          "text": "<span style='color:#1f77b4; font-size:20px;'>â—</span> XGBoost (Default)",
          "x": 1.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.88,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "left",
          "font": {
           "color": "#2c3e50",
           "size": 16
          },
          "showarrow": false,
          "text": "<span style='color:#2ca02c; font-size:20px;'>â—</span> LightGBM",
          "x": 1.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.76,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "left",
          "font": {
           "color": "#2c3e50",
           "size": 16
          },
          "showarrow": false,
          "text": "<span style='color:#9467bd; font-size:20px;'>â—</span> Random Forest",
          "x": 1.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.64,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "left",
          "font": {
           "color": "#2c3e50",
           "size": 16
          },
          "showarrow": false,
          "text": "<span style='color:#E7B142; font-size:20px;'>â—</span> Logistic Regression",
          "x": 1.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.52,
          "yanchor": "top",
          "yref": "paper"
         }
        ],
        "height": 600,
        "legend": {
         "itemsizing": "constant",
         "title": {
          "text": "Model"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "l": 98,
         "r": 270
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 22
         },
         "text": "<b>The Trade-off: Accuracy vs. Confidence</b><br><sup>Top-Left is the 'Sweet Spot'. Tuning pushed XGBoost further into the ideal zone.</sup>"
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "range": [
          0.13024000000000002,
          0.0043
         ],
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 16
          },
          "text": "<b>Log Loss (Uncertainty) â†’ Lower is Better</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 16
          },
          "text": "<b>F1-Score (Accuracy) â†’ Higher is Better</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "All 2 figures saved successfully in 'figures/' directory\n",
      "Ready for presentation!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create directory for saving figures\n",
    "FIGURES_DIR = \"figures/\"\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# --- PREPARE DATA ---\n",
    "# Re-construct the data\n",
    "leaderboard_data = {\n",
    "    \"Model\": [\"LightGBM\", \"XGBoost (Default)\", \"Random Forest\", \"Logistic Regression\"],\n",
    "    \"Val_F1\": [0.9967, 0.9964, 0.9951, 0.9506],\n",
    "    \"Val_LogLoss\": [0.0097, 0.0092, 0.0229, 0.1184]\n",
    "}\n",
    "df_lead = pd.DataFrame(leaderboard_data)\n",
    "\n",
    "new_row = {\n",
    "    \"Model\": \"XGBoost (Tuned v2)\", \n",
    "    \"Val_F1\": 0.9971, \n",
    "    \"Val_LogLoss\": 0.0086\n",
    "}\n",
    "df_final_viz = pd.concat([df_lead, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df_final_viz = df_final_viz.sort_values(\"Val_LogLoss\", ascending=True)\n",
    "\n",
    "# --- FIGURE 4: IMPROVED BAR CHART (Better Gradient) ---\n",
    "# Custom scale: Start at Dark Blue (#08306b) -> End at Light Blue (#bdd7e7)\n",
    "# This prevents the lightest bar from becoming white/invisible.\n",
    "custom_blue_scale = [\n",
    "    (0.0, \"#08306b\"), # Lowest LogLoss (Best) -> Darkest Blue\n",
    "    (1.0, \"#bdd7e7\")  # Highest LogLoss (Worst) -> Lightest Blue (Visible)\n",
    "]\n",
    "\n",
    "fig4 = px.bar(\n",
    "    df_final_viz,\n",
    "    x=\"Val_LogLoss\",\n",
    "    y=\"Model\",\n",
    "    orientation=\"h\",\n",
    "    title=\"<b>Model Confidence (Log Loss): Lower is Better</b><br><sup>The Tuned XGBoost achieves the highest certainty (Lowest Error).</sup>\",\n",
    "    text_auto=\".4f\",\n",
    "    color=\"Val_LogLoss\",\n",
    "    color_continuous_scale=custom_blue_scale\n",
    ")\n",
    "\n",
    "fig4.update_layout(\n",
    "    height=600,\n",
    "    width=1400,\n",
    "    xaxis_title=\"<b>Validation Log Loss (Lower = Better)</b>\",\n",
    "    yaxis_title=\"<b>Model</b>\",\n",
    "    showlegend=False,\n",
    "    coloraxis_showscale=True,  # Show the colorbar for Val_LogLoss\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=dict(text=\"Val LogLoss\", side=\"right\", font=dict(size=16)),\n",
    "        tickfont=dict(size=14)\n",
    "    ),\n",
    "    margin=dict(r=100, l=230),\n",
    "    title_font_size=22,\n",
    "    xaxis_title_font_size=16,\n",
    "    yaxis_title_font_size=16,\n",
    "    xaxis_tickfont_size=16,\n",
    "    yaxis_tickfont_size=16\n",
    ")\n",
    "\n",
    "# Save figure 4 at high quality\n",
    "fig4.write_image(f\"{FIGURES_DIR}figure4_model_leaderboard.png\", scale=3, width=1400, height=600)\n",
    "print(f\"âœ“ Figure 4 saved to {FIGURES_DIR}figure4_model_leaderboard.png\")\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "# --- FIGURE 5: ACCURACY vs CONFIDENCE (Clean Sidebar Version) ---\n",
    "\n",
    "# 1. Sort Data by Performance (Best to Worst) so the list is ordered\n",
    "# We sort by Log Loss (Ascending) -> Lowest Loss is First\n",
    "df_viz_sorted = df_final_viz.sort_values(\"Val_LogLoss\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "# 2. Define Colors\n",
    "model_colors = {\n",
    "    \"Logistic Regression\": \"#E7B142\", # Yellow/Orange\n",
    "    \"Random Forest\": \"#9467bd\",       # Purple\n",
    "    \"LightGBM\": \"#2ca02c\",            # Green\n",
    "    \"XGBoost (Default)\": \"#1f77b4\",   # Blue\n",
    "    \"XGBoost (Tuned v2)\": \"#d62728\"   # Red (Champion)\n",
    "}\n",
    "\n",
    "fig5 = px.scatter(\n",
    "    df_viz_sorted,\n",
    "    x=\"Val_LogLoss\",\n",
    "    y=\"Val_F1\",\n",
    "    color=\"Model\",\n",
    "    size=[70, 50, 50, 45, 45], # Champion gets biggest dot\n",
    "    opacity=0.75,\n",
    "    color_discrete_map=model_colors,\n",
    "    title=\"<b>The Trade-off: Accuracy vs. Confidence</b><br><sup>Top-Left is the 'Sweet Spot'. Tuning pushed XGBoost further into the ideal zone.</sup>\",\n",
    "    hover_data=[\"Model\", \"Val_F1\", \"Val_LogLoss\"]\n",
    ")\n",
    "\n",
    "# 3. Create Custom Sidebar Annotations\n",
    "annotations = []\n",
    "# Start Y position at the top (1.0) and move down\n",
    "y_pos = 1.0 \n",
    "y_step = 0.12 # Space between items\n",
    "\n",
    "for index, row in df_viz_sorted.iterrows():\n",
    "    model_name = row[\"Model\"]\n",
    "    color = model_colors.get(model_name, \"black\")\n",
    "    \n",
    "    # Add Champion Icon for the winner\n",
    "    if \"Tuned\" in model_name:\n",
    "        display_text = f\"<b>ðŸ† {model_name}</b>\"\n",
    "    else:\n",
    "        display_text = f\"{model_name}\"\n",
    "\n",
    "    # Use HTML to color the dot inside the text\n",
    "    # Unicode Circle: â—\n",
    "    annotation_text = f\"<span style='color:{color}; font-size:20px;'>â—</span> {display_text}\"\n",
    "\n",
    "    annotations.append(dict(\n",
    "        x=1.02, # Just outside the right edge of the plot area\n",
    "        y=y_pos,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=annotation_text,\n",
    "        showarrow=False,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "        align=\"left\",\n",
    "        font=dict(size=16, color=\"#2c3e50\")\n",
    "    ))\n",
    "    \n",
    "    y_pos -= y_step # Move down for next item\n",
    "\n",
    "fig5.update_layout(\n",
    "    height=600,\n",
    "    width=1400, # Wider to fit the sidebar\n",
    "    xaxis_title=\"<b>Log Loss (Uncertainty) â†’ Lower is Better</b>\",\n",
    "    yaxis_title=\"<b>F1-Score (Accuracy) â†’ Higher is Better</b>\",\n",
    "    xaxis=dict(autorange=\"reversed\"), # Best (Low Loss) on Right (or Left depending on preference, usually Left is 0) -> Let's keep Low Loss on Left\n",
    "    showlegend=False, # We built our own custom legend\n",
    "    annotations=annotations,\n",
    "    margin=dict(r=270, l=98), # Large Right Margin to hold the text list\n",
    "    title_font_size=22,\n",
    "    xaxis_title_font_size=16,\n",
    "    yaxis_title_font_size=16,\n",
    "    xaxis_tickfont_size=16,\n",
    "    yaxis_tickfont_size=16\n",
    ")\n",
    "\n",
    "# Force X-axis range to breathe if dots are on edge\n",
    "min_loss = df_viz_sorted['Val_LogLoss'].min()\n",
    "max_loss = df_viz_sorted['Val_LogLoss'].max()\n",
    "fig5.update_xaxes(range=[max_loss * 1.1, min_loss * 0.5]) # Reversed range manual control\n",
    "\n",
    "# Save figure 5 at high quality\n",
    "fig5.write_image(f\"{FIGURES_DIR}figure5_tuning_lift.png\", scale=3, width=1400, height=600)\n",
    "print(f\"âœ“ Figure 5 saved to {FIGURES_DIR}figure5_tuning_lift.png\")\n",
    "\n",
    "fig5.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All 2 figures saved successfully in '{FIGURES_DIR}' directory\")\n",
    "print(f\"Ready for presentation!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
