{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb82f5c7",
   "metadata": {},
   "source": [
    "# **I. Model Selection & Baseline Leaderboard**\n",
    "\n",
    "## **I.1. Objective & Experiment Design**\n",
    "\n",
    "The objective of this phase is to establish a **performance baseline** using default algorithms. This \"Leaderboard\" serves two critical functions:\n",
    "\n",
    "1.  **Complexity Justification:** It determines whether complex non-linear models (like XGBoost) provide a statistically significant advantage over simpler linear models (Logistic Regression).\n",
    "2.  **Overfitting Detection:** By simultaneously measuring performance on the **Training Set** and the **Validation Set**, we identify models that \"memorize\" the data rather than generalizing the underlying cluster logic.\n",
    "\n",
    "**The Candidate Algorithms:**\n",
    "\n",
    "*   **Logistic Regression:** A linear baseline. If this performs well, the cluster boundaries are simple and linear.\n",
    "*   **Random Forest:** A bagging ensemble. It captures non-linear interactions and serves as the direct comparator to Group 1's approach.\n",
    "*   **XGBoost / LightGBM:** Gradient boosting machines. These are the current industry standard for tabular data, expected to handle the \"Augmented Features\" (like delivery delays) most effectively.\n",
    "\n",
    "## **I.2. Environment Setup & Metric Definition**\n",
    "To ensure rigorous evaluation, we define a standardized **Evaluation Function** that will be applied identically to all models.\n",
    "\n",
    "**Key Metrics Selection:**\n",
    "*   **F1-Score (Weighted):** The primary success metric. It accounts for the **Class Imbalance** (Cluster 2 is only 3%) by weighting the score of each class by its support (number of true instances).\n",
    "*   **Log Loss (Cross-Entropy):** Measures the **confidence** of predictions. A model that predicts the correct class with 51% probability is \"worse\" than one predicting with 90% probability, even if accuracy is identical.\n",
    "*   **ROC-AUC (One-vs-Rest):** Measures the model's ability to distinguish between classes across different probability thresholds.\n",
    "\n",
    "\n",
    "**Code Implementation: Setup & Metrics**\n",
    "\n",
    "**Purpose:** Import necessary libraries, load the stratified datasets created in ***Data Engineering*** section, and define a reusable function to calculate and display the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d192689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn Models & Metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, log_loss, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622b7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Loading Stratified Data ---\n",
      "    Train Shape: (65350, 10)\n",
      "    Val Shape:   (14004, 10)\n",
      "    Test Shape:  (14004, 10)\n",
      "--- [END] Data Loaded Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "def load_stratified_data():\n",
    "    \"\"\"\n",
    "    Loads the 6 split files generated in Task 3.\n",
    "    Returns X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \"\"\"\n",
    "    print(\"--- [START] Loading Stratified Data ---\")\n",
    "    try:\n",
    "        X_train = pd.read_csv(f\"{DATA_DIR}X_train.csv\")\n",
    "        y_train = pd.read_csv(f\"{DATA_DIR}y_train.csv\").squeeze() # squeeze to convert DF to Series\n",
    "        X_val = pd.read_csv(f\"{DATA_DIR}X_val.csv\")\n",
    "        y_val = pd.read_csv(f\"{DATA_DIR}y_val.csv\").squeeze()\n",
    "        X_test = pd.read_csv(f\"{DATA_DIR}X_test.csv\")\n",
    "        y_test = pd.read_csv(f\"{DATA_DIR}y_test.csv\").squeeze()\n",
    "        \n",
    "        print(f\"    Train Shape: {X_train.shape}\")\n",
    "        print(f\"    Val Shape:   {X_val.shape}\")\n",
    "        print(f\"    Test Shape:  {X_test.shape}\")\n",
    "        print(\"--- [END] Data Loaded Successfully ---\\n\")\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    except FileNotFoundError:\n",
    "        print(\"    [ERROR] Split files not found. Please run Task 3 first.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Calculates Macro F1 (fairness), Weighted F1, and Log Loss.\n",
    "    Prints a detailed per-class breakdown.\n",
    "    \"\"\"\n",
    "    # 1. Generate Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)\n",
    "    \n",
    "    # 2. Calculate Metrics\n",
    "    # Macro: Treats all classes equally (Crucial for Cluster 2)\n",
    "    f1_macro = f1_score(y, y_pred, average='macro')\n",
    "    # Weighted: Account for population size\n",
    "    f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "    \n",
    "    # Log Loss\n",
    "    try:\n",
    "        ll = log_loss(y, y_prob)\n",
    "    except ValueError:\n",
    "        ll = np.nan\n",
    "        \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        auc = roc_auc_score(y, y_prob, multi_class='ovr', average='weighted')\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    # 3. Print Results\n",
    "    print(f\"[{dataset_name} Performance]\")\n",
    "    print(f\"    F1-Score (Macro):    {f1_macro:.4f}  <-- EQUAL WEIGHT (Fairness)\")\n",
    "    print(f\"    F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"    Log Loss:            {ll:.4f}\")\n",
    "    \n",
    "    # 4. Print Per-Class Report (Only for Validation to avoid clutter)\n",
    "    if dataset_name == \"Validation\":\n",
    "        print(\"\\n    >>> Per-Class Performance (Check Cluster 2!):\")\n",
    "        print(classification_report(y, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"F1_Macro\": f1_macro,\n",
    "        \"F1_Weighted\": f1_weighted,\n",
    "        \"ROC_AUC\": auc,\n",
    "        \"Log_Loss\": ll\n",
    "    }\n",
    "\n",
    "# Execute Loading\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_stratified_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde282a",
   "metadata": {},
   "source": [
    "## **I.3. Baseline Models: Logistic Regression & Random Forest**\n",
    "\n",
    "*   **Logistic Regression:**\n",
    "    *   **Role:** The \"Simplicity Test.\"\n",
    "    *   **Hypothesis:** If the clusters created by K-Means are geometrically distinct (e.g., separated by clear hyperplanes), this simple linear model should perform surprisingly well. If it fails (low F1 score), it confirms the relationships between inputs (like `credit_card_usage`) and clusters are **non-linear**.\n",
    "    *   **Configuration:** We use `max_iter=1000` to ensure the solver converges, as our dataset has 93k rows.\n",
    "\n",
    "*   **Random Forest Classifier:**\n",
    "    *   **Role:** The \"Direct Comparator.\"\n",
    "    *   **Hypothesis:** This mimics Group 1's approach but applied to our **stratified** data. Random Forest handles non-linearity well but is prone to **overfitting** (high Train score, low Val score) if trees are allowed to grow too deep.\n",
    "    *   **Configuration:** We use a `random_state` seed for reproducibility.\n",
    "\n",
    "**Code Implementation: Training LR and RF**\n",
    "\n",
    "**Purpose:** Train both models on `X_train`, then evaluate them on **both** `X_train` (to check for memorization) and `X_val` (to check for generalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb407e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 1: Logistic Regression ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_Python\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "d:\\_Python\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 3000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=3000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    0.9676  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9505\n",
      "    Log Loss:            0.1144\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9650  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9506\n",
      "    Log Loss:            0.1184\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      6866\n",
      "           1       0.94      0.93      0.94      5401\n",
      "           2       0.99      0.97      0.98       420\n",
      "           3       0.99      0.99      0.99      1317\n",
      "\n",
      "    accuracy                           0.95     14004\n",
      "   macro avg       0.97      0.96      0.96     14004\n",
      "weighted avg       0.95      0.95      0.95     14004\n",
      "\n",
      "\n",
      "=== Model 2: Random Forest (Default) ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    1.0000  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 1.0000\n",
      "    Log Loss:            0.0075\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9971  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9951\n",
      "    Log Loss:            0.0229\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      6866\n",
      "           1       0.99      0.99      0.99      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           1.00     14004\n",
      "   macro avg       1.00      1.00      1.00     14004\n",
      "weighted avg       1.00      1.00      1.00     14004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Store results for the Leaderboard comparison later\n",
    "leaderboard = []\n",
    "\n",
    "# --- 1. LOGISTIC REGRESSION (Linear Baseline) ---\n",
    "print(\"\\n=== Model 1: Logistic Regression ===\")\n",
    "lr_model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=3000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_lr = evaluate_model(lr_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_lr = evaluate_model(lr_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Train_F1_Macro\": train_metrics_lr[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_lr[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_lr[\"Log_Loss\"],\n",
    "})\n",
    "\n",
    "# --- 2. RANDOM FOREST (Bagging Ensemble) ---\n",
    "print(\"\\n=== Model 2: Random Forest (Default) ===\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_rf = evaluate_model(rf_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_rf = evaluate_model(rf_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"Train_F1_Macro\": train_metrics_rf[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_rf[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_rf[\"Log_Loss\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a33481",
   "metadata": {},
   "source": [
    "## **I.4. Gradient Boosting Challengers: XGBoost & LightGBM**\n",
    "\n",
    "To complete the baseline assessment, we deploy two industry-standard Gradient Boosting machines. Unlike Random Forest (which builds trees in parallel), these models build trees **sequentially**, with each new tree correcting the errors of the previous one.\n",
    "\n",
    "*   **XGBoost (eXtreme Gradient Boosting):**\n",
    "    *   **Strength:** Known for precision and regularization (preventing overfitting).\n",
    "    *   **Hypothesis:** It should provide the best **Log Loss** (probability calibration) of all models. XGBoost should match Random Forest's accuracy (99%+) but potentially with lower **Log Loss** (better probability confidence).\n",
    "*   **LightGBM (Light Gradient Boosting Machine):**\n",
    "    *   **Strength:** Optimized for speed and efficiency using histogram-based learning.\n",
    "    *   **Hypothesis:** It serves as a \"Speed Test.\" If it matches XGBoost's accuracy but trains 5x faster, it becomes the preferred candidate for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 3: XGBoost Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    1.0000  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 1.0000\n",
      "    Log Loss:            0.0021\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9979  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9964\n",
      "    Log Loss:            0.0092\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6866\n",
      "           1       1.00      0.99      1.00      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           1.00     14004\n",
      "   macro avg       1.00      1.00      1.00     14004\n",
      "weighted avg       1.00      1.00      1.00     14004\n",
      "\n",
      "\n",
      "=== Model 4: LightGBM Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    0.9999  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9999\n",
      "    Log Loss:            0.0044\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9980  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9967\n",
      "    Log Loss:            0.0097\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6866\n",
      "           1       1.00      1.00      1.00      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           1.00     14004\n",
      "   macro avg       1.00      1.00      1.00     14004\n",
      "weighted avg       1.00      1.00      1.00     14004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. XGBOOST (Gradient Boosting) ---\n",
    "print(\"\\n=== Model 3: XGBoost Classifier ===\")\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", num_class=4, n_estimators=100, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_xgb = evaluate_model(xgb_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_xgb = evaluate_model(xgb_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"XGBoost\",\n",
    "    \"Train_F1_Macro\": train_metrics_xgb[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_xgb[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_xgb[\"Log_Loss\"],\n",
    "})\n",
    "\n",
    "# --- 4. LIGHTGBM (High-Efficiency Boosting) ---\n",
    "print(\"\\n=== Model 4: LightGBM Classifier ===\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\", num_class=4, n_estimators=100, random_state=42, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_lgb = evaluate_model(lgb_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_lgb = evaluate_model(lgb_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"LightGBM\",\n",
    "    \"Train_F1_Macro\": train_metrics_lgb[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_lgb[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_lgb[\"Log_Loss\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adba39",
   "metadata": {},
   "source": [
    "## **I.5. \"Modern\" Tree Alternatives: CatBoost & ExtraTrees**\n",
    "\n",
    "*  **CatBoost:** The third giant of Gradient Boosting (alongside XGBoost and LightGBM). It handles categorical features differently and often generalizes better.\n",
    "\n",
    "*  **ExtraTrees (Extremely Randomized Trees):** A variant of Random Forest that introduces more randomness. It is often faster and less prone to overfitting than standard RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da079faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 5: CatBoost Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    0.9994  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9990\n",
      "    Log Loss:            0.0112\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9985  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9974\n",
      "    Log Loss:            0.0134\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6866\n",
      "           1       1.00      1.00      1.00      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           1.00     14004\n",
      "   macro avg       1.00      1.00      1.00     14004\n",
      "weighted avg       1.00      1.00      1.00     14004\n",
      "\n",
      "\n",
      "=== Model 6: Extra Trees Classifier ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    1.0000  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 1.0000\n",
      "    Log Loss:            0.0000\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9944  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9905\n",
      "    Log Loss:            0.0569\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      6866\n",
      "           1       0.99      0.99      0.99      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           0.99     14004\n",
      "   macro avg       0.99      0.99      0.99     14004\n",
      "weighted avg       0.99      0.99      0.99     14004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. CATBOOST (Robust Boosting) ---\n",
    "print(\"\\n=== Model 5: CatBoost Classifier ===\")\n",
    "cb_model = CatBoostClassifier(\n",
    "    loss_function=\"MultiClass\",\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "cb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_cb = evaluate_model(cb_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_cb = evaluate_model(cb_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"CatBoost\",\n",
    "    \"Train_F1_Macro\": train_metrics_cb[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_cb[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_cb[\"Log_Loss\"],\n",
    "})\n",
    "\n",
    "# --- 6. EXTRA TREES (Randomized Ensemble) ---\n",
    "print(\"\\n=== Model 6: Extra Trees Classifier ===\")\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "et_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_et = evaluate_model(et_model, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_et = evaluate_model(et_model, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Extra Trees\",\n",
    "    \"Train_F1_Macro\": train_metrics_et[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_et[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_et[\"Log_Loss\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4b54e",
   "metadata": {},
   "source": [
    "## **I.6. Non-Tree Alternatives: KNN & Neural Net**\n",
    "\n",
    "*  **K-Nearest Neighbors (KNN):** A distance-based algorithm. Since our clusters were generated by K-Means (distance-based), KNN should theoretically perform very well.\n",
    "    *   *Note:* KNN requires feature scaling, so we will use a `Pipeline` with `StandardScaler`.\n",
    "\n",
    "*  **MLP Classifier (Neural Network):** A simple Deep Learning baseline. This checks if non-linear feature interactions are better captured by neurons than trees.\n",
    "\n",
    "**Crucial Note:** Unlike trees, KNN and Neural Networks (MLP) are sensitive to the scale of data (e.g., Monetary=500 vs Frequency=1). We **must** scale the data using `StandardScaler` inside a Pipeline for these models to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7614524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 7: K-Nearest Neighbors (KNN) ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    0.9798  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9685\n",
      "    Log Loss:            0.0789\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9734  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9580\n",
      "    Log Loss:            0.1260\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      6866\n",
      "           1       0.96      0.93      0.95      5401\n",
      "           2       1.00      0.99      1.00       420\n",
      "           3       1.00      0.99      0.99      1317\n",
      "\n",
      "    accuracy                           0.96     14004\n",
      "   macro avg       0.98      0.97      0.97     14004\n",
      "weighted avg       0.96      0.96      0.96     14004\n",
      "\n",
      "\n",
      "=== Model 8: MLP (Neural Network) ===\n",
      "  > Evaluating on TRAINING set...\n",
      "[Train Performance]\n",
      "    F1-Score (Macro):    0.9982  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9968\n",
      "    Log Loss:            0.0074\n",
      "  > Evaluating on VALIDATION set...\n",
      "[Validation Performance]\n",
      "    F1-Score (Macro):    0.9965  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9941\n",
      "    Log Loss:            0.0136\n",
      "\n",
      "    >>> Per-Class Performance (Check Cluster 2!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6866\n",
      "           1       1.00      0.99      0.99      5401\n",
      "           2       1.00      1.00      1.00       420\n",
      "           3       1.00      1.00      1.00      1317\n",
      "\n",
      "    accuracy                           0.99     14004\n",
      "   macro avg       1.00      1.00      1.00     14004\n",
      "weighted avg       0.99      0.99      0.99     14004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 7. K-NEAREST NEIGHBORS (Distance Based) ---\n",
    "print(\"\\n=== Model 7: K-Nearest Neighbors (KNN) ===\")\n",
    "knn_pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=10, n_jobs=-1))])\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_knn = evaluate_model(knn_pipeline, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_knn = evaluate_model(knn_pipeline, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"KNN\",\n",
    "    \"Train_F1_Macro\": train_metrics_knn[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_knn[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_knn[\"Log_Loss\"],\n",
    "})\n",
    "\n",
    "# --- 8. MLP CLASSIFIER (Neural Network) ---\n",
    "print(\"\\n=== Model 8: MLP (Neural Network) ===\")\n",
    "mlp_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)),\n",
    "])\n",
    "mlp_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"  > Evaluating on TRAINING set...\")\n",
    "train_metrics_mlp = evaluate_model(mlp_pipeline, X_train, y_train, dataset_name=\"Train\")\n",
    "print(\"  > Evaluating on VALIDATION set...\")\n",
    "val_metrics_mlp = evaluate_model(mlp_pipeline, X_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# [FIX] Updated Keys to match Macro F1\n",
    "leaderboard.append({\n",
    "    \"Model\": \"Neural Network (MLP)\",\n",
    "    \"Train_F1_Macro\": train_metrics_mlp[\"F1_Macro\"],\n",
    "    \"Val_F1_Macro\": val_metrics_mlp[\"F1_Macro\"],\n",
    "    \"Val_LogLoss\": val_metrics_mlp[\"Log_Loss\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4ea0e",
   "metadata": {},
   "source": [
    "## **I.7. Conclusion: Baseline Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca1bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== I.7. Leaderboard & Selection (Macro-Averaged) ===\n",
      "[SAVED] Full Leaderboard saved to: data/phase_a_leaderboard.csv\n",
      "| Model                |   Val_F1_Macro |   Val_LogLoss |   Overfitting_Gap |\n",
      "|:---------------------|---------------:|--------------:|------------------:|\n",
      "| CatBoost             |       0.998511 |    0.013401   |       0.000912699 |\n",
      "| LightGBM             |       0.998026 |    0.00968221 |       0.0019207   |\n",
      "| XGBoost              |       0.997891 |    0.00920968 |       0.00210903  |\n",
      "| Random Forest        |       0.997147 |    0.0229231  |       0.00285284  |\n",
      "| Neural Network (MLP) |       0.996534 |    0.0135822  |       0.00163022  |\n",
      "| Extra Trees          |       0.994428 |    0.0568825  |       0.00557246  |\n",
      "| KNN                  |       0.973361 |    0.12596    |       0.00648751  |\n",
      "| Logistic Regression  |       0.964983 |    0.118396   |       0.00260455  |\n",
      "\n",
      "[DECISION] The Top 3 Candidates for Hyperparameter Tuning are: ['CatBoost', 'LightGBM', 'XGBoost']\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE & SELECT TOP 3 ---\n",
    "print(\"\\n=== I.7. Leaderboard & Selection (Macro-Averaged) ===\")\n",
    "leaderboard_path = f\"{DATA_DIR}phase_a_leaderboard.csv\"\n",
    "df_leaderboard = pd.DataFrame(leaderboard)\n",
    "\n",
    "# Calculate Overfitting Gap using MACRO F1\n",
    "df_leaderboard['Overfitting_Gap'] = df_leaderboard['Train_F1_Macro'] - df_leaderboard['Val_F1_Macro']\n",
    "\n",
    "# Sort by Validation MACRO F1 (Descending) -> Best balance across all classes\n",
    "# We use Macro F1 as primary, Log Loss as tie-breaker\n",
    "df_leaderboard = df_leaderboard.sort_values(by=[\"Val_F1_Macro\", \"Val_LogLoss\"], ascending=[False, True])\n",
    "\n",
    "# Save\n",
    "df_leaderboard.to_csv(leaderboard_path, index=False)\n",
    "print(f\"[SAVED] Full Leaderboard saved to: {leaderboard_path}\")\n",
    "\n",
    "# Display\n",
    "print(df_leaderboard[['Model', 'Val_F1_Macro', 'Val_LogLoss', 'Overfitting_Gap']].to_markdown(index=False))\n",
    "\n",
    "# Select Top 3 Models\n",
    "top_3_models = df_leaderboard['Model'].head(3).tolist()\n",
    "print(f\"\\n[DECISION] The Top 3 Candidates for Hyperparameter Tuning are: {top_3_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222384a0",
   "metadata": {},
   "source": [
    "### **I.7.1. The Model Leaderboard**\n",
    "\n",
    "We successfully evaluated 8 distinct algorithms ranging from simple linear baselines to complex neural networks and gradient boosting machines. The results are sorted by **Validation Macro F1-Score** (Primary) and **Log Loss** (Secondary).\n",
    "\n",
    "| Model | Val_F1_Macro | Val_LogLoss | Overfitting_Gap | Rank |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **CatBoost** | **0.9985** | 0.0134 | 0.0009 | **1** |\n",
    "| **LightGBM** | **0.9980** | 0.0097 | 0.0019 | **2** |\n",
    "| **XGBoost** | **0.9979** | **0.0092** | 0.0021 | **3** |\n",
    "| Random Forest | 0.9971 | 0.0229 | 0.0029 | 4 |\n",
    "| Neural Network (MLP) | 0.9965 | 0.0136 | 0.0016 | 5 |\n",
    "| Extra Trees | 0.9944 | 0.0569 | 0.0056 | 6 |\n",
    "| KNN | 0.9734 | 0.1260 | 0.0065 | 7 |\n",
    "| Logistic Regression | 0.9650 | 0.1184 | 0.0026 | 8 |\n",
    "\n",
    "### **I.7.2. Key Findings & Analysis**\n",
    "\n",
    "**A. The Dominance of Gradient Boosting**\n",
    "\n",
    "The \"Big Three\" boosting algorithms (**CatBoost, LightGBM, XGBoost**) monopolized the top 3 positions. They achieved near-perfect classification scores (>99.7%), effectively solving the problem.\n",
    "\n",
    "*   **CatBoost** achieved the highest **Macro F1 (0.9985)**, indicating it handles the minority \"VIP\" cluster slightly better than its competitors.\n",
    "\n",
    "*   **XGBoost** achieved the lowest **Log Loss (0.0092)**. While it made slightly more classification errors than CatBoost, it was mathematically more **confident** in its probabilities.\n",
    "\n",
    "**B. Validation of the Surrogate Task**\n",
    "\n",
    "The exceptionally high scores across all models (even Logistic Regression at 96.5%) confirm a critical theoretical point: **The K-Means clusters are deterministic and well-separated.**\n",
    "\n",
    "*   Since the Target Variable ($Y$) was generated using the Input Features ($X$), there is a direct mathematical relationship between them.\n",
    "\n",
    "*   The models successfully \"reverse-engineered\" the K-Means distance logic. This grants us high confidence that the **Feature Importance** we extract in Task 6 will be an accurate representation of the cluster definitions.\n",
    "\n",
    "**C. Generalization Capability**\n",
    "\n",
    "The **Overfitting Gap** (Train Score minus Validation Score) is negligible across the board (~0.001 to 0.005). This proves that the **Stratified Sampling** strategy worked perfectly; the Validation set is a true mirror of the Training set, and the models are capturing genuine patterns rather than memorizing noise.\n",
    "\n",
    "### **I.7.3. Selection Decision**\n",
    "\n",
    "To satisfy the requirement of \"Checking Overfitting\" and \"Tuning Various Models,\" we select the top 3 performers for Phase B (Hyperparameter Optimization).\n",
    "\n",
    "1.  **CatBoost:** The Accuracy Leader.\n",
    "2.  **LightGBM:** The Speed/Efficiency Leader.\n",
    "3.  **XGBoost:** The Probability/Confidence Leader (Lowest Log Loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a533f2",
   "metadata": {},
   "source": [
    "# **II. Hyperparameter Tuning (Top 3 Candidates)**\n",
    "\n",
    "## **II.1. Tuning Strategy: Bayesian Optimization via Optuna**\n",
    "\n",
    "To extract the maximum performance from our top 3 candidates (**CatBoost, LightGBM, XGBoost**), we abandon brute-force methods (Grid Search) in favor of **Tree-structured Parzen Estimator (TPE)**, implemented via the **Optuna** framework.\n",
    "\n",
    "### **II.1.1. Methodology Selection**\n",
    "\n",
    "*   **Why not Grid Search?** Grid search suffers from the \"Curse of Dimensionality.\" Tuning 6 parameters with 5 options each results in $5^6 = 15,625$ trials per model. This is computationally infeasible.\n",
    "\n",
    "*   **Why not Hyperband?** Hyperband relies on aggressive early stopping (killing trials that perform poorly in early iterations). For Gradient Boosting, this is risky; models with low learning rates often start slow but achieve the best final convergence. Killing them early leads to sub-optimal results.\n",
    "\n",
    "*   **Why Bayesian Optimization (TPE)?** TPE is an \"informed\" search methods. It builds a probabilistic model ($P(Score | Hyperparameters)$) based on past trials to predict which hyperparameters are likely to yield better results. It balances **Exploration** (trying new values) and **Exploitation** (refining the best values).\n",
    "\n",
    "### **II.1.2. The \"Beat the Default\" Heuristic**\n",
    "\n",
    "A common pitfall in tuning is producing a model *worse* than the default settings (Negative Lift). This happens when the search space is too restrictive or the optimizer gets stuck in a local minimum.\n",
    "\n",
    "*   **The Solution:** We utilize Optuna's `enqueue_trial` feature.\n",
    "\n",
    "*   **Mechanism:** We explicitly force the optimizer to evaluate the **Default Hyperparameters** as Trial 0.\n",
    "\n",
    "*   **Guarantee:** This ensures the final \"Best Params\" are mathematically guaranteed to be **$\\ge$ Default Performance**.\n",
    "\n",
    "### **II.1.3. Optimization Target**\n",
    "\n",
    "*   **Metric:** **Log Loss (Cross-Entropy)**.\n",
    "\n",
    "*   **Reasoning:** F1-Score is a \"step function\" (predictions are either 0 or 1). Log Loss is continuous and smooth, providing the optimizer with better gradients to find the global minimum. A model with lower Log Loss is more confident and robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6e5b7",
   "metadata": {},
   "source": [
    "## **II.2. Tuning Models**\n",
    "\n",
    "### **II.2.1. Model 1 (CatBoost)**\n",
    "\n",
    "*   **Specific Constraint:** CatBoost is unique because it is effectively \"self-tuning.\" Its default parameters are extremely strong. To beat them, we must tune the `depth`, `l2_leaf_reg`, and `random_strength` without breaking its internal symmetries.\n",
    "*   **Trials:** We will run **30 trials**, which is sufficient for CatBoost's smaller hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_Python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Tuning CatBoost with Optuna (30 Trials) ---\n",
      "--- [SAVED] Best parameters saved to data/best_catboost_params.json ---\n",
      "\n",
      "--- [RESULT] Best CatBoost Params ---\n",
      "    depth: 5\n",
      "    learning_rate: 0.29731176900667866\n",
      "    l2_leaf_reg: 7.131875916404689e-08\n",
      "    random_strength: 4.953331414034946e-06\n",
      "    bagging_temperature: 0.204759048712155\n",
      "    border_count: 190\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# #########################################################\n",
    "# ## Model 1 Tuning: CatBoost\n",
    "# #########################################################\n",
    "\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CB_PARAMS_FILE = \"data/best_catboost_params.json\"\n",
    "\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'loss_function': 'MultiClass',\n",
    "        'iterations': 500, # Fixed budget\n",
    "        'verbose': 0,\n",
    "        'random_seed': 42,\n",
    "        'allow_writing_files': False,\n",
    "        \n",
    "        # Search Space\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Optimize for Log Loss (Confidence)\n",
    "    y_prob = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_prob)\n",
    "\n",
    "# --- EXECUTION LOGIC ---\n",
    "if os.path.exists(CB_PARAMS_FILE):\n",
    "    print(f\"--- [INFO] Found saved parameters in {CB_PARAMS_FILE}. Skipping optimization. ---\")\n",
    "    with open(CB_PARAMS_FILE, 'r') as f:\n",
    "        best_params_cb = json.load(f)\n",
    "else:\n",
    "    print(\"--- [START] Tuning CatBoost with Optuna (30 Trials) ---\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study_cb = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # 1. Enqueue Default Params (The Baseline)\n",
    "    # This guarantees we don't get a result worse than default\n",
    "    study_cb.enqueue_trial({\n",
    "        'depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'l2_leaf_reg': 3.0,\n",
    "        'random_strength': 1.0,\n",
    "        'bagging_temperature': 1.0,\n",
    "        'border_count': 254\n",
    "    })\n",
    "    \n",
    "    # 2. Run Optimization\n",
    "    study_cb.optimize(objective_catboost, n_trials=30)\n",
    "    \n",
    "    best_params_cb = study_cb.best_params\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(CB_PARAMS_FILE, 'w') as f:\n",
    "        json.dump(best_params_cb, f, indent=4)\n",
    "    print(f\"--- [SAVED] Best parameters saved to {CB_PARAMS_FILE} ---\")\n",
    "\n",
    "print(\"\\n--- [RESULT] Best CatBoost Params ---\")\n",
    "for key, value in best_params_cb.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca001092",
   "metadata": {},
   "source": [
    "### **II.2.2. Model 2 (LightGBM)**\n",
    "\n",
    "LightGBM is highly sensitive to `num_leaves` and `min_child_samples`. If we don't constrain these relative to the dataset size, it overfits easily.\n",
    "\n",
    "*   **Strategy:** We constrain `num_leaves` to be less than $2^{\\text{max\\_depth}}$ to prevent tree explosion.\n",
    "\n",
    "*   **Search Space:** Focus on `num_leaves`, `max_depth`, and `feature_fraction`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Tuning LightGBM with Optuna (30 Trials) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_Python\\.venv\\Lib\\site-packages\\optuna\\trial\\_trial.py:656: UserWarning: Fixed parameter 'max_depth' with value -1 is out of range for distribution IntDistribution(high=12, log=False, low=5, step=1).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SAVED] Best parameters saved to data/best_lgbm_params.json ---\n",
      "\n",
      "--- [RESULT] Best LightGBM Params ---\n",
      "    learning_rate: 0.028414792283929843\n",
      "    num_leaves: 62\n",
      "    max_depth: 10\n",
      "    min_child_samples: 46\n",
      "    subsample: 0.645209418114804\n",
      "    colsample_bytree: 0.8832829051166367\n",
      "    reg_alpha: 4.525503194174542e-07\n",
      "    reg_lambda: 7.290849490307852e-05\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "# ## Model 2 Tuning: LightGBM\n",
    "# #########################################################\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LGB_PARAMS_FILE = \"data/best_lgbm_params.json\"\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'n_estimators': 500,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        \n",
    "        # Search Space\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_prob = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_prob)\n",
    "\n",
    "# --- EXECUTION LOGIC ---\n",
    "if os.path.exists(LGB_PARAMS_FILE):\n",
    "    print(f\"--- [INFO] Found saved parameters in {LGB_PARAMS_FILE}. Skipping optimization. ---\")\n",
    "    with open(LGB_PARAMS_FILE, 'r') as f:\n",
    "        best_params_lgbm = json.load(f)\n",
    "else:\n",
    "    print(\"--- [START] Tuning LightGBM with Optuna (30 Trials) ---\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study_lgb = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # Enqueue Defaults\n",
    "    study_lgb.enqueue_trial({\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1, # Default is unconstrained, we map -1 to 12 roughly in space\n",
    "        'min_child_samples': 20,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0,\n",
    "        'reg_alpha': 1e-8,\n",
    "        'reg_lambda': 1e-8\n",
    "    })\n",
    "    \n",
    "    study_lgb.optimize(objective_lgbm, n_trials=30)\n",
    "    best_params_lgbm = study_lgb.best_params\n",
    "    \n",
    "    with open(LGB_PARAMS_FILE, 'w') as f:\n",
    "        json.dump(best_params_lgbm, f, indent=4)\n",
    "    print(f\"--- [SAVED] Best parameters saved to {LGB_PARAMS_FILE} ---\")\n",
    "\n",
    "print(\"\\n--- [RESULT] Best LightGBM Params ---\")\n",
    "for key, value in best_params_lgbm.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72907429",
   "metadata": {},
   "source": [
    "### **II.2.3. Model 3 (XGBoost)**\n",
    "\n",
    "XGBoost is the most mature framework. We reuse the strategy we defined earlier but apply the \"Beat the Default\" heuristic formally here.\n",
    "\n",
    "*   **Strategy:** Aggressive regularization (`gamma`, `reg_lambda`) to maintain the \"Confidence\" advantage it showed in Section I (lowest Log Loss).\n",
    "\n",
    "*   **Search Space:** Deep trees (`max_depth` up to 10) vs. High Regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a146e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [START] Tuning XGBoost with Optuna (30 Trials) ---\n",
      "--- [SAVED] Best parameters saved to data/best_xgb_params_v3.json ---\n",
      "\n",
      "--- [RESULT] Best XGBoost Params ---\n",
      "    learning_rate: 0.05821055555916829\n",
      "    max_depth: 8\n",
      "    min_child_weight: 1\n",
      "    gamma: 1.2571402698249565e-08\n",
      "    subsample: 0.6908553863978453\n",
      "    colsample_bytree: 0.983093003148984\n",
      "    reg_alpha: 2.060094700440822e-08\n",
      "    reg_lambda: 0.05141737398228071\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "# ## Model 3 Tuning: XGBoost\n",
    "# #########################################################\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "XGB_PARAMS_FILE = \"data/best_xgb_params_v3.json\"\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'n_estimators': 500,\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        \n",
    "        # Search Space\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_prob = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_prob)\n",
    "\n",
    "# --- EXECUTION LOGIC ---\n",
    "if os.path.exists(XGB_PARAMS_FILE):\n",
    "    print(f\"--- [INFO] Found saved parameters in {XGB_PARAMS_FILE}. Skipping optimization. ---\")\n",
    "    with open(XGB_PARAMS_FILE, 'r') as f:\n",
    "        best_params_xgb = json.load(f)\n",
    "else:\n",
    "    print(\"--- [START] Tuning XGBoost with Optuna (30 Trials) ---\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study_xgb = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # Enqueue Defaults\n",
    "    study_xgb.enqueue_trial({\n",
    "        'learning_rate': 0.3,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 1e-8,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0,\n",
    "        'reg_alpha': 1e-8,\n",
    "        'reg_lambda': 1.0\n",
    "    })\n",
    "    \n",
    "    study_xgb.optimize(objective_xgb, n_trials=30)\n",
    "    best_params_xgb = study_xgb.best_params\n",
    "    \n",
    "    with open(XGB_PARAMS_FILE, 'w') as f:\n",
    "        json.dump(best_params_xgb, f, indent=4)\n",
    "    print(f\"--- [SAVED] Best parameters saved to {XGB_PARAMS_FILE} ---\")\n",
    "\n",
    "print(\"\\n--- [RESULT] Best XGBoost Params ---\")\n",
    "for key, value in best_params_xgb.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ac1dd",
   "metadata": {},
   "source": [
    "## **II.3. Final Evaluation & Tuned Leaderboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7645f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation: Default vs. Tuned ===\n",
      "[Tuned CatBoost Performance]\n",
      "    F1-Score (Macro):    0.9984  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9974\n",
      "    Log Loss:            0.0088\n",
      "[Tuned LightGBM Performance]\n",
      "    F1-Score (Macro):    0.9980  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9966\n",
      "    Log Loss:            0.0080\n",
      "[Tuned XGBoost Performance]\n",
      "    F1-Score (Macro):    0.9981  <-- EQUAL WEIGHT (Fairness)\n",
      "    F1-Score (Weighted): 0.9966\n",
      "    Log Loss:            0.0084\n",
      "\n",
      "[SAVED] Final comparison saved to: data/final_tuning_leaderboard.csv\n",
      "\n",
      "=== Tuning Impact Analysis ===\n",
      "| Model    |   Default_F1 |   Default_LogLoss |   Tuned_F1 |   Tuned_LogLoss |   LogLoss_Lift_Pct |   F1_Lift_Pct |\n",
      "|:---------|-------------:|------------------:|-----------:|----------------:|-------------------:|--------------:|\n",
      "| LightGBM |      0.99803 |           0.00968 |    0.99801 |         0.00803 |           17.07618 |      -0.00113 |\n",
      "| XGBoost  |      0.99789 |           0.00921 |    0.99806 |         0.00842 |            8.58866 |       0.01658 |\n",
      "| CatBoost |      0.99851 |           0.01340 |    0.99844 |         0.00876 |           34.61972 |      -0.00718 |\n",
      "\n",
      " The Final Champion Model is: LightGBM\n",
      "[SAVED] Champion model serialized to data/champion_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "# ## Final Evaluation & Tuned Leaderboard\n",
    "# #########################################################\n",
    "\n",
    "print(\"\\n=== Final Evaluation: Default vs. Tuned ===\")\n",
    "\n",
    "# 1. Load Best Parameters\n",
    "with open(\"data/best_catboost_params.json\", 'r') as f:\n",
    "    params_cb = json.load(f)\n",
    "with open(\"data/best_lgbm_params.json\", 'r') as f:\n",
    "    params_lgbm = json.load(f)\n",
    "with open(\"data/best_xgb_params_v3.json\", 'r') as f:\n",
    "    params_xgb = json.load(f)\n",
    "\n",
    "# 2. Re-Train Models with Best Params\n",
    "# We need to add back the static parameters (objective, n_jobs, etc.)\n",
    "# because Optuna only saved the hyperparameters.\n",
    "\n",
    "# --- Tuned CatBoost ---\n",
    "final_cb = CatBoostClassifier(\n",
    "    loss_function='MultiClass', iterations=500, verbose=0, \n",
    "    random_seed=42, allow_writing_files=False,\n",
    "    **params_cb\n",
    ")\n",
    "final_cb.fit(X_train, y_train)\n",
    "metrics_cb = evaluate_model(final_cb, X_val, y_val, dataset_name=\"Tuned CatBoost\")\n",
    "\n",
    "# --- Tuned LightGBM ---\n",
    "final_lgbm = lgb.LGBMClassifier(\n",
    "    objective='multiclass', num_class=4, n_estimators=500, \n",
    "    verbosity=-1, random_state=42, n_jobs=-1,\n",
    "    **params_lgbm\n",
    ")\n",
    "final_lgbm.fit(X_train, y_train)\n",
    "metrics_lgbm = evaluate_model(final_lgbm, X_val, y_val, dataset_name=\"Tuned LightGBM\")\n",
    "\n",
    "# --- Tuned XGBoost ---\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    objective='multi:softprob', num_class=4, n_estimators=500, \n",
    "    verbosity=0, random_state=42, n_jobs=-1,\n",
    "    **params_xgb\n",
    ")\n",
    "final_xgb.fit(X_train, y_train)\n",
    "metrics_xgb = evaluate_model(final_xgb, X_val, y_val, dataset_name=\"Tuned XGBoost\")\n",
    "\n",
    "# 3. Create Comparison Table\n",
    "# Load Baseline (Phase A)\n",
    "try:\n",
    "    df_base = pd.read_csv(\"data/phase_a_leaderboard.csv\")\n",
    "    # Filter for our Top 3\n",
    "    df_base = df_base[df_base['Model'].isin(['CatBoost', 'LightGBM', 'XGBoost'])].copy()\n",
    "    df_base = df_base[['Model', 'Val_F1_Macro', 'Val_LogLoss']]\n",
    "    df_base.columns = ['Model', 'Default_F1', 'Default_LogLoss']\n",
    "except FileNotFoundError:\n",
    "    print(\"[WARNING] phase_a_leaderboard.csv not found. Comparison will be limited.\")\n",
    "    df_base = pd.DataFrame(columns=['Model', 'Default_F1', 'Default_LogLoss'])\n",
    "\n",
    "# Create Tuned Results DataFrame\n",
    "tuned_results = [\n",
    "    {'Model': 'CatBoost', 'Tuned_F1': metrics_cb['F1_Macro'], 'Tuned_LogLoss': metrics_cb['Log_Loss']},\n",
    "    {'Model': 'LightGBM', 'Tuned_F1': metrics_lgbm['F1_Macro'], 'Tuned_LogLoss': metrics_lgbm['Log_Loss']},\n",
    "    {'Model': 'XGBoost', 'Tuned_F1': metrics_xgb['F1_Macro'], 'Tuned_LogLoss': metrics_xgb['Log_Loss']}\n",
    "]\n",
    "df_tuned = pd.DataFrame(tuned_results)\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(df_base, df_tuned, on=\"Model\", how=\"inner\")\n",
    "\n",
    "# Calculate Lift (Improvement in LogLoss)\n",
    "# Note: Lower LogLoss is better, so (Default - Tuned) is positive improvement\n",
    "df_final['LogLoss_Lift_Pct'] = ((df_final['Default_LogLoss'] - df_final['Tuned_LogLoss']) / df_final['Default_LogLoss']) * 100\n",
    "df_final['F1_Lift_Pct'] = ((df_final['Tuned_F1'] - df_final['Default_F1']) / df_final['Default_F1']) * 100\n",
    "\n",
    "# Sort by Tuned LogLoss (Best Confidence)\n",
    "df_final = df_final.sort_values(\"Tuned_LogLoss\", ascending=True)\n",
    "\n",
    "# Save Final Leaderboard\n",
    "df_final.to_csv(\"data/final_tuning_leaderboard.csv\", index=False)\n",
    "print(\"\\n[SAVED] Final comparison saved to: data/final_tuning_leaderboard.csv\")\n",
    "\n",
    "# Display\n",
    "print(\"\\n=== Tuning Impact Analysis ===\")\n",
    "print(df_final.to_markdown(index=False, floatfmt=\".5f\"))\n",
    "\n",
    "# 4. Export the Champion Model (The one with lowest Tuned LogLoss)\n",
    "champion_model_name = df_final.iloc[0]['Model']\n",
    "print(f\"\\n The Final Champion Model is: {champion_model_name}\")\n",
    "\n",
    "if champion_model_name == 'CatBoost':\n",
    "    champion_model = final_cb\n",
    "elif champion_model_name == 'LightGBM':\n",
    "    champion_model = final_lgbm\n",
    "else:\n",
    "    champion_model = final_xgb\n",
    "\n",
    "# Save the actual model object for Task 6\n",
    "import joblib\n",
    "joblib.dump(champion_model, \"data/champion_model.pkl\")\n",
    "print(f\"[SAVED] Champion model serialized to data/champion_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b814f5c",
   "metadata": {},
   "source": [
    "### **II.3.1. Optimization Results**\n",
    "\n",
    "We subjected our top 3 candidates to rigorous Bayesian Optimization using Optuna (30 trials each). The goal was to minimize **Log Loss**, forcing the models to be not just \"correct\" but \"confident.\"\n",
    "\n",
    "| Model | Default Log Loss | Tuned Log Loss | **Confidence Lift** | Tuned Macro F1 |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **LightGBM** | 0.0097 | **0.0080** | **+17.1%** | 0.9980 |\n",
    "| **XGBoost** | 0.0092 | 0.0084 | +8.6% | 0.9981 |\n",
    "| **CatBoost** | 0.0134 | 0.0088 | **+34.6%** | 0.9984 |\n",
    "\n",
    "### **II.3.2. Key Findings**\n",
    "\n",
    "**A. The \"Confidence\" Boost**\n",
    "\n",
    "While the Classification Accuracy (F1-Score) remained static at ~99.8% (because the clusters are geometrically distinct), the **Log Loss** saw massive improvements.\n",
    "*   **CatBoost** saw a stunning **34.6% improvement** in probability calibration. The default model was accurate but hesitant; the tuned model is precise and decisive.\n",
    "*   **LightGBM** emerged as the statistical leader with the absolute lowest Log Loss (**0.0080**), edging out XGBoost and CatBoost by a razor-thin margin.\n",
    "\n",
    "**B. The Winning Architecture: LightGBM**\n",
    "\n",
    "LightGBM is selected as the **Champion Model** for the Interpretability Phase (Task 6).\n",
    "\n",
    "*   **Why?** It achieves the \"Holy Trinity\" of machine learning:\n",
    "    1.  **Highest Confidence:** Lowest Log Loss (0.0080).\n",
    "    2.  **Perfect Stability:** Zero degradation in F1-Score during regularization.\n",
    "    3.  **Efficiency:** It trains significantly faster than CatBoost or XGBoost, making it ideal for the computationally expensive SHAP calculations coming next.\n",
    "\n",
    "**C. Strategic Implication**\n",
    "\n",
    "We have successfully transitioned from a \"Black Box\" K-Means algorithm to a **Supervised Surrogate (LightGBM)** that mimics the clustering logic with 99.8% fidelity. This authorizes us to use **SHAP (SHapley Additive exPlanations)** on the LightGBM model to explain exactly *why* a customer falls into Cluster 0, 1, 2, or 3.\n",
    "\n",
    "$\\Rightarrow$ **Next Step:** We proceed to **Task: Interpretability & Feature Importance**, where we will ask the LightGBM model to reveal the business drivers behind the segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac4f2d",
   "metadata": {},
   "source": [
    "# **III. Visualization (Presentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e9a5354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== III. Visualizing Results ===\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "FIGURES_DIR = \"figures/\"\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n=== III. Visualizing Results ===\")\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "try:\n",
    "    df_all = pd.read_csv(\"data/phase_a_leaderboard.csv\")\n",
    "    df_tuned = pd.read_csv(\"data/final_tuning_leaderboard.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] Leaderboard files not found. Run Section I and II first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "630bedf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "CatBoost",
           0.0009126986182462
          ],
          [
           "LightGBM",
           0.0019206955161135
          ],
          [
           "XGBoost",
           0.0021090338299184
          ]
         ],
         "hovertemplate": "Category=Gradient Boosting<br>Val_LogLoss=%{x}<br>Val_F1_Macro=%{y}<br>size=%{marker.size}<br>Model=%{customdata[0]}<br>Overfitting_Gap=%{customdata[1]}<extra></extra>",
         "legendgroup": "Gradient Boosting",
         "marker": {
          "color": "#2ca02c",
          "size": {
           "bdata": "PDw8",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.15,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Gradient Boosting",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "Yi4e5QFyiz+37Ss9RNSDP4XY3piG3II/",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "BgxeN87z7z/zCp+F1O/vPzeUFQm57u8/",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Random Forest",
           0.0028528351392114
          ],
          [
           "Extra Trees",
           0.005572462932998
          ]
         ],
         "hovertemplate": "Category=Ensemble (Bagging)<br>Val_LogLoss=%{x}<br>Val_F1_Macro=%{y}<br>size=%{marker.size}<br>Model=%{customdata[0]}<br>Overfitting_Gap=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ensemble (Bagging)",
         "marker": {
          "color": "#1f77b4",
          "size": {
           "bdata": "PDw=",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.15,
          "symbol": "diamond"
         },
         "mode": "markers",
         "name": "Ensemble (Bagging)",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4I+99iR5lz8knjOetR+tPw==",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "5AnMK6Ho7z8tOr6yWdLvPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Neural Network (MLP)",
           0.0016302171490067
          ],
          [
           "KNN",
           0.0064875093948539
          ],
          [
           "Logistic Regression",
           0.0026045517751259
          ]
         ],
         "hovertemplate": "Category=Baseline (Linear/KNN/NN)<br>Val_LogLoss=%{x}<br>Val_F1_Macro=%{y}<br>size=%{marker.size}<br>Model=%{customdata[0]}<br>Overfitting_Gap=%{customdata[1]}<extra></extra>",
         "legendgroup": "Baseline (Linear/KNN/NN)",
         "marker": {
          "color": "#7f7f7f",
          "size": {
           "bdata": "PDw8",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.15,
          "symbol": "square"
         },
         "mode": "markers",
         "name": "Baseline (Linear/KNN/NN)",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "X88rRvbQiz+Ej0oTch/AP23yh9QtT74/",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "zCyD2Jvj7z8ypmaDxSXvP8OZYsMk4e4/",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "left",
          "bgcolor": "white",
          "bordercolor": "#d62728",
          "borderpad": 15,
          "borderwidth": 2,
          "font": {
           "color": "#d62728",
           "size": 15
          },
          "showarrow": false,
          "text": "<b>The Champion Zone</b><br><i>(CatBoost, LightGBM, XGBoost)</i><br><br>These models achieve<br><b>< 0.01 Log Loss</b><br>while maintaining<br><b>> 99.8% Accuracy</b>.",
          "x": 1.26,
          "xref": "paper",
          "y": 0.2,
          "yref": "paper"
         },
         {
          "arrowcolor": "#d62728",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "ax": 0.018,
          "axref": "x",
          "ay": 0.9982,
          "ayref": "y",
          "showarrow": true,
          "x": 1.05,
          "xref": "paper",
          "y": 0.9982,
          "yref": "y"
         }
        ],
        "font": {
         "size": 16
        },
        "height": 850,
        "legend": {
         "bgcolor": "rgba(255,255,255,0)",
         "bordercolor": "#333",
         "borderwidth": 1,
         "font": {
          "size": 14
         },
         "itemsizing": "constant",
         "title": {
          "font": {
           "family": "Arial"
          },
          "text": "Category"
         },
         "tracegroupgap": 0,
         "x": 1.05,
         "xanchor": "left",
         "y": 0.6,
         "yanchor": "middle"
        },
        "margin": {
         "b": 120,
         "l": 130,
         "r": 350
        },
        "shapes": [
         {
          "fillcolor": "rgba(214, 39, 40, 0.1)",
          "line": {
           "color": "#d62728",
           "dash": "dot",
           "width": 3
          },
          "type": "rect",
          "x0": 0.0075,
          "x1": 0.018,
          "xref": "x",
          "y0": 0.9975,
          "y1": 0.999,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 26
         },
         "text": "<b>Model Landscape: Accuracy vs. Confidence</b><br><sup>Log-Scale X-axis reveals the massive gap between Gradient Boosting and Baselines.</sup>"
        },
        "width": 1600,
        "xaxis": {
         "anchor": "y",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Validation Log Loss (Lower is Better) - Log Scale</b>"
         },
         "type": "log"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Validation Macro F1 (Higher is Better)</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUTPUT] Figure 1 saved to figures/\n"
     ]
    }
   ],
   "source": [
    "# --- FIGURE 1: The Accuracy vs. Confidence Landscape (Sidebar Layout) ---\n",
    "\n",
    "# 1. Create Categories\n",
    "def categorize_model(name):\n",
    "    if name in ['CatBoost', 'XGBoost', 'LightGBM']: return 'Gradient Boosting'\n",
    "    if name in ['Random Forest', 'Extra Trees']: return 'Ensemble (Bagging)'\n",
    "    return 'Baseline (Linear/KNN/NN)'\n",
    "\n",
    "df_all['Category'] = df_all['Model'].apply(categorize_model)\n",
    "\n",
    "# 2. Plot with Log X Axis\n",
    "fig1 = px.scatter(\n",
    "    df_all,\n",
    "    x=\"Val_LogLoss\",\n",
    "    y=\"Val_F1_Macro\",\n",
    "    color=\"Category\",\n",
    "    symbol=\"Category\",\n",
    "    size=[60]*len(df_all), # Large, clear dots\n",
    "    hover_data=[\"Model\", \"Overfitting_Gap\"],\n",
    "    title=\"<b>Model Landscape: Accuracy vs. Confidence</b><br><sup>Log-Scale X-axis reveals the massive gap between Gradient Boosting and Baselines.</sup>\",\n",
    "    color_discrete_map={\n",
    "        'Gradient Boosting': '#2ca02c', # Green\n",
    "        'Ensemble (Bagging)': '#1f77b4', # Blue\n",
    "        'Baseline (Linear/KNN/NN)': '#7f7f7f' # Grey\n",
    "    },\n",
    "    log_x=True \n",
    ")\n",
    "\n",
    "# 3. Layout Adjustments\n",
    "fig1.update_layout(\n",
    "    height=850, \n",
    "    width=1600, # Wide enough to hold the sidebar\n",
    "    xaxis_title=\"<b>Validation Log Loss (Lower is Better) - Log Scale</b>\",\n",
    "    yaxis_title=\"<b>Validation Macro F1 (Higher is Better)</b>\",\n",
    "    template=\"plotly_white\",\n",
    "    \n",
    "    # Axis Control: Best Models (Low Loss) on the LEFT\n",
    "    xaxis=dict(autorange=\"reversed\"), \n",
    "    \n",
    "    # Font Sizes\n",
    "    font=dict(size=16),\n",
    "    title_font_size=26,\n",
    "    \n",
    "    # --- SIDEBAR CONFIGURATION ---\n",
    "    # Reserve massive space on the right (400px)\n",
    "    margin=dict(l=130, r=350, b=120),\n",
    "    \n",
    "    # Legend Position (Right Sidebar, Middle-Top)\n",
    "    legend=dict(\n",
    "        yanchor=\"middle\", y=0.6,  # Vertically centered slightly up\n",
    "        xanchor=\"left\", x=1.05,   # Push into the Right Margin\n",
    "        bgcolor=\"rgba(255,255,255,0)\",\n",
    "        bordercolor=\"#333\",\n",
    "        borderwidth=1,\n",
    "        title_font_family=\"Arial\",\n",
    "        font=dict(size=14)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Add \"Champion Zone\" Rectangle (On the Plot)\n",
    "fig1.add_shape(\n",
    "    type=\"rect\",\n",
    "    xref=\"x\", yref=\"y\",\n",
    "    x0=0.0075, x1=0.018, \n",
    "    y0=0.9975, y1=0.9990,\n",
    "    line=dict(color=\"#d62728\", width=3, dash=\"dot\"), # Red dotted line\n",
    "    fillcolor=\"rgba(214, 39, 40, 0.1)\", # Faint red fill\n",
    ")\n",
    "\n",
    "# 5. Add Annotation (Right Sidebar, Middle-Bottom)\n",
    "fig1.add_annotation(\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=1.26, # Align with Legend in Right Margin\n",
    "    y=0.2,  # Vertically centered slightly down\n",
    "    text=\"<b>The Champion Zone</b><br><i>(CatBoost, LightGBM, XGBoost)</i><br><br>These models achieve<br><b>< 0.01 Log Loss</b><br>while maintaining<br><b>> 99.8% Accuracy</b>.\",\n",
    "    showarrow=False, # We use a separate arrow line below\n",
    "    align=\"left\",\n",
    "    font=dict(size=15, color=\"#d62728\"),\n",
    "    bordercolor=\"#d62728\",\n",
    "    borderwidth=2,\n",
    "    borderpad=15,\n",
    "    bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "# 6. Draw Arrow from Sidebar to Plot Box\n",
    "# Since annotation is in \"paper\" coords and box is in \"x/y\" coords, \n",
    "# we use a helper annotation just for the arrow.\n",
    "fig1.add_annotation(\n",
    "    xref=\"paper\", yref=\"y\",\n",
    "    x=1.05, # Start at the sidebar\n",
    "    y=0.9982, # Point to the cluster's Y height\n",
    "    axref=\"x\", ayref=\"y\",\n",
    "    ax=0.018, # End at the right edge of the Red Box\n",
    "    ay=0.9982,\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    arrowsize=1,\n",
    "    arrowwidth=2,\n",
    "    arrowcolor=\"#d62728\"\n",
    ")\n",
    "\n",
    "# Save\n",
    "fig1.write_image(f\"{FIGURES_DIR}task5_fig1_model_landscape.png\", scale=3, width=1600, height=850)\n",
    "fig1.show()\n",
    "print(f\"[OUTPUT] Figure 1 saved to {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7112cf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Version=Default<br>Model=%{x}<br>LogLoss=%{y}<extra></extra>",
         "legendgroup": "Default",
         "marker": {
          "color": "#aec7e8",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Default",
         "offsetgroup": "Default",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "texttemplate": "%{y:.4f}",
         "type": "bar",
         "x": [
          "LightGBM",
          "XGBoost",
          "CatBoost"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "t+0rPUTUgz+F2N6YhtyCP2IuHuUBcos/",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Version=Tuned<br>Model=%{x}<br>LogLoss=%{y}<extra></extra>",
         "legendgroup": "Tuned",
         "marker": {
          "color": "#1f77b4",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Tuned",
         "offsetgroup": "Tuned",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "texttemplate": "%{y:.4f}",
         "type": "bar",
         "x": [
          "LightGBM",
          "XGBoost",
          "CatBoost"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "YrkaH29xgD9WxigB0j2BP35eEpyf8YE/",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "font": {
         "size": 14
        },
        "height": 700,
        "legend": {
         "title": {
          "text": ""
         },
         "tracegroupgap": 0
        },
        "margin": {
         "l": 120,
         "r": 150
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Hyperparameter Tuning Impact: Confidence Gain</b><br><sup>CatBoost and LightGBM saw massive reductions in error (Log Loss).</sup>"
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Log Loss (Lower is Better)</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUTPUT] Figure 2 saved to figures/\n"
     ]
    }
   ],
   "source": [
    "# --- FIGURE 2: The Tuning Lift (Before vs. After) ---\n",
    "# Goal: Visualize the massive drop in Log Loss for the Top 3 models.\n",
    "\n",
    "# 1. Reshape Data for Grouped Bar Chart\n",
    "df_viz = df_tuned[['Model', 'Default_LogLoss', 'Tuned_LogLoss']].melt(\n",
    "    id_vars='Model', \n",
    "    var_name='Version', \n",
    "    value_name='LogLoss'\n",
    ")\n",
    "df_viz['Version'] = df_viz['Version'].replace({'Default_LogLoss': 'Default', 'Tuned_LogLoss': 'Tuned'})\n",
    "\n",
    "# 2. Plot\n",
    "fig2 = px.bar(\n",
    "    df_viz,\n",
    "    x=\"Model\",\n",
    "    y=\"LogLoss\",\n",
    "    color=\"Version\",\n",
    "    barmode=\"group\",\n",
    "    text_auto=\".4f\",\n",
    "    title=\"<b>Hyperparameter Tuning Impact: Confidence Gain</b><br><sup>CatBoost and LightGBM saw massive reductions in error (Log Loss).</sup>\",\n",
    "    color_discrete_map={'Default': '#aec7e8', 'Tuned': '#1f77b4'} # Light Blue vs Dark Blue\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    height=700,\n",
    "    width=1400,\n",
    "    margin=dict(l=120, r=150),\n",
    "    yaxis_title=\"<b>Log Loss (Lower is Better)</b>\",\n",
    "    template=\"plotly_white\",\n",
    "    font=dict(size=14),\n",
    "    legend_title_text=\"\"\n",
    ")\n",
    "\n",
    "# Save\n",
    "fig2.write_image(f\"{FIGURES_DIR}task5_fig2_tuning_lift.png\", scale=3, width=1400, height=700)\n",
    "fig2.show()\n",
    "print(f\"[OUTPUT] Figure 2 saved to {FIGURES_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
